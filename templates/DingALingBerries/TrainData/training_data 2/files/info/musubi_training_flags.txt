
usage: wan_train_network.py [-h] [--config_file CONFIG_FILE] [--dataset_config DATASET_CONFIG] [--sdpa] [--flash_attn] [--sage_attn]
                            [--xformers] [--flash3] [--split_attn] [--max_train_steps MAX_TRAIN_STEPS]
                            [--max_train_epochs MAX_TRAIN_EPOCHS] [--max_data_loader_n_workers MAX_DATA_LOADER_N_WORKERS]
                            [--persistent_data_loader_workers] [--seed SEED] [--gradient_checkpointing]
                            [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS] [--mixed_precision {no,fp16,bf16}]
                            [--logging_dir LOGGING_DIR] [--log_with {tensorboard,wandb,all}] [--log_prefix LOG_PREFIX]
                            [--log_tracker_name LOG_TRACKER_NAME] [--wandb_run_name WANDB_RUN_NAME]
                            [--log_tracker_config LOG_TRACKER_CONFIG] [--wandb_api_key WANDB_API_KEY] [--log_config]
                            [--ddp_timeout DDP_TIMEOUT] [--ddp_gradient_as_bucket_view] [--ddp_static_graph]
                            [--sample_every_n_steps SAMPLE_EVERY_N_STEPS] [--sample_at_first]
                            [--sample_every_n_epochs SAMPLE_EVERY_N_EPOCHS] [--sample_prompts SAMPLE_PROMPTS]
                            [--optimizer_type OPTIMIZER_TYPE] [--optimizer_args [OPTIMIZER_ARGS ...]] [--learning_rate LEARNING_RATE]
                            [--max_grad_norm MAX_GRAD_NORM] [--lr_scheduler LR_SCHEDULER] [--lr_warmup_steps LR_WARMUP_STEPS]
                            [--lr_decay_steps LR_DECAY_STEPS] [--lr_scheduler_num_cycles LR_SCHEDULER_NUM_CYCLES]
                            [--lr_scheduler_power LR_SCHEDULER_POWER] [--lr_scheduler_timescale LR_SCHEDULER_TIMESCALE]
                            [--lr_scheduler_min_lr_ratio LR_SCHEDULER_MIN_LR_RATIO] [--lr_scheduler_type LR_SCHEDULER_TYPE]
                            [--lr_scheduler_args [LR_SCHEDULER_ARGS ...]] [--fp8_base]
                            [--dynamo_backend {NO,EAGER,AOT_EAGER,INDUCTOR,AOT_TS_NVFUSER,NVPRIMS_NVFUSER,CUDAGRAPHS,OFI,FX2TRT,ONNXRT,TENSORRT,AOT_TORCHXLA_TRACE_ONCE,TORCHXLA_TRACE_ONCE,IPEX,TVM,HPU_BACKEND}]
                            [--dynamo_mode {default,reduce-overhead,max-autotune}] [--dynamo_fullgraph] [--dynamo_dynamic]
                            [--blocks_to_swap BLOCKS_TO_SWAP] [--img_in_txt_in_offloading] [--guidance_scale GUIDANCE_SCALE]
                            [--timestep_sampling {sigma,uniform,sigmoid,shift,flux_shift,logsnr,qinglong}]
                            [--discrete_flow_shift DISCRETE_FLOW_SHIFT] [--sigmoid_scale SIGMOID_SCALE]
                            [--weighting_scheme {logit_normal,mode,cosmap,sigma_sqrt,none}] [--logit_mean LOGIT_MEAN]
                            [--logit_std LOGIT_STD] [--mode_scale MODE_SCALE] [--min_timestep MIN_TIMESTEP] [--max_timestep MAX_TIMESTEP]
                            [--preserve_distribution_shape] [--show_timesteps {image,console}] [--no_metadata]
                            [--network_weights NETWORK_WEIGHTS] [--network_module NETWORK_MODULE] [--network_dim NETWORK_DIM]
                            [--network_alpha NETWORK_ALPHA] [--network_dropout NETWORK_DROPOUT] [--network_args [NETWORK_ARGS ...]]
                            [--training_comment TRAINING_COMMENT] [--dim_from_weights] [--scale_weight_norms SCALE_WEIGHT_NORMS]
                            [--base_weights [BASE_WEIGHTS ...]] [--base_weights_multiplier [BASE_WEIGHTS_MULTIPLIER ...]]
                            [--output_dir OUTPUT_DIR] [--output_name OUTPUT_NAME] [--resume RESUME]
                            [--save_every_n_epochs SAVE_EVERY_N_EPOCHS] [--save_every_n_steps SAVE_EVERY_N_STEPS]
                            [--save_last_n_epochs SAVE_LAST_N_EPOCHS] [--save_last_n_epochs_state SAVE_LAST_N_EPOCHS_STATE]
                            [--save_last_n_steps SAVE_LAST_N_STEPS] [--save_last_n_steps_state SAVE_LAST_N_STEPS_STATE] [--save_state]
                            [--save_state_on_train_end] [--metadata_title METADATA_TITLE] [--metadata_author METADATA_AUTHOR]
                            [--metadata_description METADATA_DESCRIPTION] [--metadata_license METADATA_LICENSE]
                            [--metadata_tags METADATA_TAGS] [--huggingface_repo_id HUGGINGFACE_REPO_ID]
                            [--huggingface_repo_type HUGGINGFACE_REPO_TYPE] [--huggingface_path_in_repo HUGGINGFACE_PATH_IN_REPO]
                            [--huggingface_token HUGGINGFACE_TOKEN] [--huggingface_repo_visibility HUGGINGFACE_REPO_VISIBILITY]
                            [--save_state_to_huggingface] [--resume_from_huggingface] [--async_upload] [--dit DIT] [--vae VAE]
                            [--vae_dtype VAE_DTYPE]
                            [--task {t2v-14B,t2v-1.3B,i2v-14B,t2i-14B,flf2v-14B,t2v-1.3B-FC,t2v-14B-FC,i2v-14B-FC,i2v-A14B,t2v-A14B}]
                            [--fp8_scaled] [--t5 T5] [--fp8_t5] [--clip CLIP] [--vae_cache_cpu] [--one_frame]
                            [--dit_high_noise DIT_HIGH_NOISE] [--timestep_boundary TIMESTEP_BOUNDARY] [--offload_inactive_dit]

options:
  -h, --help            show this help message and exit
  --config_file CONFIG_FILE
                        using .toml instead of args to pass hyperparameter / ハイパーパラメータを引数ではなく.tomlファイルで渡す
  --dataset_config DATASET_CONFIG
                        config file for dataset / データセットの設定ファイル
  --sdpa                use sdpa for CrossAttention (requires PyTorch 2.0) / CrossAttentionにsdpaを使う（PyTorch 2.0が必要）
  --flash_attn          use FlashAttention for CrossAttention, requires FlashAttention /
                        CrossAttentionにFlashAttentionを使う、FlashAttentionが必要
  --sage_attn           use SageAttention. requires SageAttention / SageAttentionを使う。SageAttentionが必要
  --xformers            use xformers for CrossAttention, requires xformers / CrossAttentionにxformersを使う、xformersが必要
  --flash3              use FlashAttention 3 for CrossAttention, requires FlashAttention 3, HunyuanVideo does not support this yet /
                        CrossAttentionにFlashAttention 3を使う、FlashAttention 3が必要。HunyuanVideoは未対応。
  --split_attn          use split attention for attention calculation (split batch size=1, affects memory usage and speed) /
                        attentionを分割して計算する（バッチサイズ=1に分割、メモリ使用量と速度に影響）
  --max_train_steps MAX_TRAIN_STEPS
                        training steps / 学習ステップ数
  --max_train_epochs MAX_TRAIN_EPOCHS
                        training epochs (overrides max_train_steps) / 学習エポック数（max_train_stepsを上書きします）
  --max_data_loader_n_workers MAX_DATA_LOADER_N_WORKERS
                        max num workers for DataLoader (lower is less main RAM usage, faster epoch start and slower data loading) /
                        DataLoaderの最大プロセス数（小さい値ではメインメモリの使用量が減りエポック間の待ち時間が減りますが、データ読み込みは遅くなります）
  --persistent_data_loader_workers
                        persistent DataLoader workers (useful for reduce time gap between epoch, but may use more memory) / DataLoader
                        のワーカーを持続させる (エポック間の時間差を少なくするのに有効だが、より多くのメモリを消費する可能性がある)
  --seed SEED           random seed for training / 学習時の乱数のseed
  --gradient_checkpointing
                        enable gradient checkpointing / gradient checkpointingを有効にする
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of updates steps to accumulate before performing a backward/update pass / 学習時に逆伝播をする前に勾配を合計 するステップ数
  --mixed_precision {no,fp16,bf16}
                        use mixed precision / 混合精度を使う場合、その精度
  --logging_dir LOGGING_DIR
                        enable logging and output TensorBoard log to this directory / ログ出力を有効にしてこのディレクトリにTensorBoard用の ログを出力する
  --log_with {tensorboard,wandb,all}
                        what logging tool(s) to use (if 'all', TensorBoard and WandB are both used) / ログ出力に使用するツール
                        (allを指定するとTensorBoardとWandBの両方が使用される)
  --log_prefix LOG_PREFIX
                        add prefix for each log directory / ログディレクトリ名の先頭に追加する文字列
  --log_tracker_name LOG_TRACKER_NAME
                        name of tracker to use for logging, default is script-specific default name /
                        ログ出力に使用するtrackerの名前、省略時はスクリプトごとのデフォルト名
  --wandb_run_name WANDB_RUN_NAME
                        The name of the specific wandb session / wandb ログに表示される特定の実行の名前
  --log_tracker_config LOG_TRACKER_CONFIG
                        path to tracker config file to use for logging / ログ出力に使用するtrackerの設定ファイルのパス
  --wandb_api_key WANDB_API_KEY
                        specify WandB API key to log in before starting training (optional). / WandB APIキーを指定して学習開始前にログインする（オプション）
  --log_config          log training configuration / 学習設定をログに出力する
  --ddp_timeout DDP_TIMEOUT
                        DDP timeout (min, None for default of accelerate) / DDPのタイムアウト（分、Noneでaccelerateのデフォルト）
  --ddp_gradient_as_bucket_view
                        enable gradient_as_bucket_view for DDP / DDPでgradient_as_bucket_viewを有効にする
  --ddp_static_graph    enable static_graph for DDP / DDPでstatic_graphを有効にする
  --sample_every_n_steps SAMPLE_EVERY_N_STEPS
                        generate sample images every N steps / 学習中のモデルで指定ステップごとにサンプル出力する
  --sample_at_first     generate sample images before training / 学習前にサンプル出力する
  --sample_every_n_epochs SAMPLE_EVERY_N_EPOCHS
                        generate sample images every N epochs (overwrites n_steps) / 学習中のモデルで指定エポックごとにサンプル出力する（ス テップ数指定を上書きします）
  --sample_prompts SAMPLE_PROMPTS
                        file for prompts to generate sample images / 学習中モデルのサンプル出力用プロンプトのファイル
  --optimizer_type OPTIMIZER_TYPE
                        Optimizer to use / オプティマイザの種類: AdamW (default), AdamW8bit, AdaFactor. Also, you can use any optimizer by
                        specifying the full path to the class, like 'torch.optim.AdamW', 'bitsandbytes.optim.AdEMAMix8bit' or
                        'bitsandbytes.optim.PagedAdEMAMix8bit' etc. /
  --optimizer_args [OPTIMIZER_ARGS ...]
                        additional arguments for optimizer (like "weight_decay=0.01 betas=0.9,0.999 ...") / オプティマイザの追加引数（例：
                        "weight_decay=0.01 betas=0.9,0.999 ..."）
  --learning_rate LEARNING_RATE
                        learning rate / 学習率
  --max_grad_norm MAX_GRAD_NORM
                        Max gradient norm, 0 for no clipping / 勾配正規化の最大norm、0でclippingを行わない
  --lr_scheduler LR_SCHEDULER
                        scheduler to use for learning rate / 学習率のスケジューラ: linear, cosine, cosine_with_restarts, polynomial, constant
                        (default), constant_with_warmup, adafactor
  --lr_warmup_steps LR_WARMUP_STEPS
                        Int number of steps for the warmup in the lr scheduler (default is 0) or float with ratio of train steps /
                        学習率のスケジューラをウォームアップするステップ数（デフォルト0）、または学習ステップの比率（1未満のfloat値の場合）
  --lr_decay_steps LR_DECAY_STEPS
                        Int number of steps for the decay in the lr scheduler (default is 0) or float (<1) with ratio of train steps /
                        学習率のスケジューラを減衰させるステップ数（デフォルト0）、または学習ステップの比率（1未満のfloat値の場合）
  --lr_scheduler_num_cycles LR_SCHEDULER_NUM_CYCLES
                        Number of restarts for cosine scheduler with restarts / cosine with restartsスケジューラでのリスタート回数
  --lr_scheduler_power LR_SCHEDULER_POWER
                        Polynomial power for polynomial scheduler / polynomialスケジューラでのpolynomial power
  --lr_scheduler_timescale LR_SCHEDULER_TIMESCALE
                        Inverse sqrt timescale for inverse sqrt scheduler,defaults to `num_warmup_steps` /
                        逆平方根スケジューラのタイムスケール、デフォルトは`num_warmup_steps`
  --lr_scheduler_min_lr_ratio LR_SCHEDULER_MIN_LR_RATIO
                        The minimum learning rate as a ratio of the initial learning rate for cosine with min lr scheduler and warmup
                        decay scheduler / 初期学習率の比率としての最小学習率を指定する、cosine with min lr と warmup decay スケジューラ で有効
  --lr_scheduler_type LR_SCHEDULER_TYPE
                        custom scheduler module / 使用するスケジューラ
  --lr_scheduler_args [LR_SCHEDULER_ARGS ...]
                        additional arguments for scheduler (like "T_max=100") / スケジューラの追加引数（例： "T_max100"）
  --fp8_base            use fp8 for base model / base modelにfp8を使う
  --dynamo_backend {NO,EAGER,AOT_EAGER,INDUCTOR,AOT_TS_NVFUSER,NVPRIMS_NVFUSER,CUDAGRAPHS,OFI,FX2TRT,ONNXRT,TENSORRT,AOT_TORCHXLA_TRACE_ONCE,TORCHXLA_TRACE_ONCE,IPEX,TVM,HPU_BACKEND}
                        dynamo backend type (default is None) / dynamoのbackendの種類（デフォルトは None）
  --dynamo_mode {default,reduce-overhead,max-autotune}
                        dynamo mode (default is default) / dynamoのモード（デフォルトは default）
  --dynamo_fullgraph    use fullgraph mode for dynamo / dynamoのfullgraphモードを使う
  --dynamo_dynamic      use dynamic mode for dynamo / dynamoのdynamicモードを使う
  --blocks_to_swap BLOCKS_TO_SWAP
                        number of blocks to swap in the model, max XXX / モデル内のブロックの数、最大XXX
  --img_in_txt_in_offloading
                        offload img_in and txt_in to cpu / img_inとtxt_inをCPUにオフロードする
  --guidance_scale GUIDANCE_SCALE
                        Embeded classifier free guidance scale (HunyuanVideo only).
  --timestep_sampling {sigma,uniform,sigmoid,shift,flux_shift,logsnr,qinglong}
                        Method to sample timesteps: sigma-based, uniform random, sigmoid of random normal, shift of sigmoid and flux
                        shift. / タイムステップをサンプリングする方法：sigma、random uniform、random normalのsigmoid、sigmoidのシフト、flux shift。
  --discrete_flow_shift DISCRETE_FLOW_SHIFT
                        Discrete flow shift for the Euler Discrete Scheduler, default is 1.0. / Euler Discrete
                        Schedulerの離散フローシフト、デフォルトは1.0。
  --sigmoid_scale SIGMOID_SCALE
                        Scale factor for sigmoid timestep sampling (only used when timestep-sampling is "sigmoid" or "shift"). /
                        sigmoidタイムステップサンプリングの倍率（timestep-samplingが"sigmoid"または"shift"の場合のみ有効）。
  --weighting_scheme {logit_normal,mode,cosmap,sigma_sqrt,none}
                        weighting scheme for timestep distribution. Default is none / タイムステップ分布の重み付けスキーム、デフォルトはnone
  --logit_mean LOGIT_MEAN
                        mean to use when using the `'logit_normal'` weighting scheme / `'logit_normal'`重み付けスキームを使用する場合の平均
  --logit_std LOGIT_STD
                        std to use when using the `'logit_normal'` weighting scheme / `'logit_normal'`重み付けスキームを使用する場合のstd
  --mode_scale MODE_SCALE
                        Scale of mode weighting scheme. Only effective when using the `'mode'` as the `weighting_scheme` /
                        モード重み付けスキームのスケール
  --min_timestep MIN_TIMESTEP
                        set minimum time step for training (0~999, default is 0) / 学習時のtime stepの最小値を設定する（0~999で指定、省略時 はデフォルト値(0)）
  --max_timestep MAX_TIMESTEP
                        set maximum time step for training (1~1000, default is 1000) / 学習時のtime stepの最大値を設定する（1~1000で指定、省略時はデフォルト値(1000)）
  --preserve_distribution_shape
                        If specified, constrains timestep sampling to [min_timestep, max_timestep] using rejection sampling, preserving
                        the original distribution shape. By default, the [0, 1] range is scaled, which distorts the distribution. Only
                        effective when `timestep_sampling` is not 'sigma'. /
                        指定すると、タイムステップのサンプリングを[最小タイムステップ、最大タイムステップ]に制約し、元の分布形状を保持します。デフォルトでは、[0,
                        1]の範囲がスケーリングされ、分布が歪むことがあります。timestep_samplingがsigma以外で有効です。
  --show_timesteps {image,console}
                        show timesteps in image or console, and return to console / タイムステップを画像またはコンソールに表示し、コンソールに戻る
  --no_metadata         do not save metadata in output model / メタデータを出力先モデルに保存しない
  --network_weights NETWORK_WEIGHTS
                        pretrained weights for network / 学習するネットワークの初期重み
  --network_module NETWORK_MODULE
                        network module to train / 学習対象のネットワークのモジュール
  --network_dim NETWORK_DIM
                        network dimensions (depends on each network) / モジュールの次元数（ネットワークにより定義は異なります）
  --network_alpha NETWORK_ALPHA
                        alpha for LoRA weight scaling, default 1 (same as network_dim for same behavior as old version) /
                        LoRaの重み調整のalpha値、デフォルト1（旧バージョンと同じ動作をするにはnetwork_dimと同じ値を指定）
  --network_dropout NETWORK_DROPOUT
                        Drops neurons out of training every step (0 or None is default behavior (no dropout), 1 would drop all neurons) /
                        訓練時に毎ステップでニューロンをdropする（0またはNoneはdropoutなし、1は全ニューロンをdropout）
  --network_args [NETWORK_ARGS ...]
                        additional arguments for network (key=value) / ネットワークへの追加の引数
  --training_comment TRAINING_COMMENT
                        arbitrary comment string stored in metadata / メタデータに記録する任意のコメント文字列
  --dim_from_weights    automatically determine dim (rank) from network_weights / dim (rank)をnetwork_weightsで指定した重みから自動で決定す る
  --scale_weight_norms SCALE_WEIGHT_NORMS
                        Scale the weight of each key pair to help prevent overtraing via exploding gradients. (1 is a good starting point)
                        / 重みの値をスケーリングして勾配爆発を防ぐ（1が初期値としては適当）
  --base_weights [BASE_WEIGHTS ...]
                        network weights to merge into the model before training / 学習前にあらかじめモデルにマージするnetworkの重みファイル
  --base_weights_multiplier [BASE_WEIGHTS_MULTIPLIER ...]
                        multiplier for network weights to merge into the model before training / 学習前にあらかじめモデルにマージするnetworkの重みの倍率
  --output_dir OUTPUT_DIR
                        directory to output trained model / 学習後のモデル出力先ディレクトリ
  --output_name OUTPUT_NAME
                        base name of trained model file / 学習後のモデルの拡張子を除くファイル名
  --resume RESUME       saved state to resume training / 学習再開するモデルのstate
  --save_every_n_epochs SAVE_EVERY_N_EPOCHS
                        save checkpoint every N epochs / 学習中のモデルを指定エポックごとに保存する
  --save_every_n_steps SAVE_EVERY_N_STEPS
                        save checkpoint every N steps / 学習中のモデルを指定ステップごとに保存する
  --save_last_n_epochs SAVE_LAST_N_EPOCHS
                        save last N checkpoints when saving every N epochs (remove older checkpoints) /
                        指定エポックごとにモデルを保存するとき最大Nエポック保存する（古いチェックポイントは削除する）
  --save_last_n_epochs_state SAVE_LAST_N_EPOCHS_STATE
                        save last N checkpoints of state (overrides the value of --save_last_n_epochs)/
                        最大Nエポックstateを保存する（--save_last_n_epochsの指定を上書きする）
  --save_last_n_steps SAVE_LAST_N_STEPS
                        save checkpoints until N steps elapsed (remove older checkpoints if N steps elapsed) /
                        指定ステップごとにモデルを保存するとき、このステップ数経過するまで保存する（このステップ数経過したら削除する）
  --save_last_n_steps_state SAVE_LAST_N_STEPS_STATE
                        save states until N steps elapsed (remove older states if N steps elapsed, overrides --save_last_n_steps) /
                        指定ステップごとにstateを保存するとき、このステップ数経過するまで保存する（このステップ数経過したら削除する。--save_last_n_stepsを上書きする）
  --save_state          save training state additionally (including optimizer states etc.) when saving model /
                        optimizerなど学習状態も含めたstateをモデル保存時に追加で保存する
  --save_state_on_train_end
                        save training state (including optimizer states etc.) on train end even if --save_state is not specified /
                        --save_stateが未指定時にもoptimizerなど学習状態も含めたstateを学習終了時に保存する
  --metadata_title METADATA_TITLE
                        title for model metadata (default is output_name) / メタデータに書き込まれるモデルタイトル、省略時はoutput_name
  --metadata_author METADATA_AUTHOR
                        author name for model metadata / メタデータに書き込まれるモデル作者名
  --metadata_description METADATA_DESCRIPTION
                        description for model metadata / メタデータに書き込まれるモデル説明
  --metadata_license METADATA_LICENSE
                        license for model metadata / メタデータに書き込まれるモデルライセンス
  --metadata_tags METADATA_TAGS
                        tags for model metadata, separated by comma / メタデータに書き込まれるモデルタグ、カンマ区切り
  --huggingface_repo_id HUGGINGFACE_REPO_ID
                        huggingface repo name to upload / huggingfaceにアップロードするリポジトリ名
  --huggingface_repo_type HUGGINGFACE_REPO_TYPE
                        huggingface repo type to upload / huggingfaceにアップロードするリポジトリの種類
  --huggingface_path_in_repo HUGGINGFACE_PATH_IN_REPO
                        huggingface model path to upload files / huggingfaceにアップロードするファイルのパス
  --huggingface_token HUGGINGFACE_TOKEN
                        huggingface token / huggingfaceのトークン
  --huggingface_repo_visibility HUGGINGFACE_REPO_VISIBILITY
                        huggingface repository visibility ('public' for public, 'private' or None for private) /
                        huggingfaceにアップロードするリポジトリの公開設定（'public'で公開、'private'またはNoneで非公開）
  --save_state_to_huggingface
                        save state to huggingface / huggingfaceにstateを保存する
  --resume_from_huggingface
                        resume from huggingface (ex: --resume {repo_id}/{path_in_repo}:{revision}:{repo_type}) / huggingfaceから学習を再開する(例:
                        --resume {repo_id}/{path_in_repo}:{revision}:{repo_type})
  --async_upload        upload to huggingface asynchronously / huggingfaceに非同期でアップロードする
  --dit DIT             DiT checkpoint path / DiTのチェックポイントのパス
  --vae VAE             VAE checkpoint path / VAEのチェックポイントのパス
  --vae_dtype VAE_DTYPE
                        data type for VAE, default is float16
  --task {t2v-14B,t2v-1.3B,i2v-14B,t2i-14B,flf2v-14B,t2v-1.3B-FC,t2v-14B-FC,i2v-14B-FC,i2v-A14B,t2v-A14B}
                        The task to run.
  --fp8_scaled          use scaled fp8 for DiT / DiTにスケーリングされたfp8を使う
  --t5 T5               text encoder (T5) checkpoint path
  --fp8_t5              use fp8 for Text Encoder model
  --clip CLIP           text encoder (CLIP) checkpoint path, optional. If training Wan2.1 I2V model, this is required
  --vae_cache_cpu       cache features in VAE on CPU
  --one_frame           Use one frame sampling method for sample generation
  --dit_high_noise DIT_HIGH_NOISE
                        DiT checkpoint path for high noise model
  --timestep_boundary TIMESTEP_BOUNDARY
                        Timestep boundary for switching between high and low noise models, defaults to None (task specific) /
                        高ノイズモデルと低ノイズモデルを切り替えるタイムステップ境界。デフォルトはNone（タスク固有）
  --offload_inactive_dit
                        Offload inactive DiT model to CPU. Cannot be used with block swap / アクティブではないDiTモデルをCPUにオフロードします。ブロックスワップと併用できません
